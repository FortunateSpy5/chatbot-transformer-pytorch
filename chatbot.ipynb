{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = './cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = './cornell movie-dialogs corpus/movie_lines.txt'\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = {}\n",
    "for line in lines:\n",
    "    objects = line.split(' +++$+++ ')\n",
    "    line_dict[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(line_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punc(s):\n",
    "    # table[, delete chars]\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in line_dict.items():\n",
    "    line_dict[k] = remove_punc(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(line_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(' +++$+++ ')[-1])\n",
    "    for i in range(len(ids) - 1):\n",
    "        qa_pairs = []\n",
    "        first = remove_punc(line_dict[ids[i]].strip())\n",
    "        second = remove_punc(line_dict[ids[i + 1]].strip())\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['<unk>'] = len(word2idx) + 1\n",
    "word2idx['<start>'] = len(word2idx) + 1\n",
    "word2idx['<end>'] = len(word2idx) + 1\n",
    "word2idx['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total no. of words are {len(word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2idx.json', 'w') as j:\n",
    "    json.dump(word2idx, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in words] + [word2idx['<pad>']] * (max_len - len(words))\n",
    "\n",
    "def encode_reply(words, word2idx):\n",
    "    return [word2idx['<start>']] + [word2idx.get(word, word2idx['<unk>']) for word in words] + [word2idx['<start>']] + [word2idx['<pad>']] * (max_len - len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = [\n",
    "    [encode_question(pair[0], word2idx), encode_reply(pair[1], word2idx)] \n",
    "    for pair in pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pairs_encoded.json', 'w') as j:\n",
    "    json.dump(pairs_encoded, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, pairs_encoded_path):\n",
    "        super().__init__()\n",
    "        self.pairs = json.load(open(pairs_encoded_path))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question = torch.LongTensor(self.pairs[index][0])\n",
    "        reply = torch.LongTensor(self.pairs[index][1])\n",
    "        return question, reply\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    DialogDataset('./pairs_encoded.json'),\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, reply = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question.size(), reply.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Understand this\n",
    "def create_masks(question, reply_input, reply_target):\n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = (question != 0).to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1) # batch_size, 1, 1, max_words\n",
    "    \n",
    "    reply_input_mask = reply_input != 0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask) # batch_size, max_words, max_words\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_target_mask = reply_target != 0\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(5, 5)).transpose(0, 1).unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Understand this 2\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positional_encoding(max_len, d_model)\n",
    "    \n",
    "    def create_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        return pe.unsqueeze_(0) # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, encoded_words):\n",
    "        embeddings = self.embed(encoded_words) * math.sqrt(self.d_model) # batch_size, max_words, d_model\n",
    "        embeddings += self.pe[:, embeddings.size(1)]\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value = (batch_size, max_words, d_model->512)\n",
    "        mask: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        query = self.query(query) # (batch_size, max_words, d_model->512)\n",
    "        key = self.key(key) # (batch_size, max_words, d_model->512)\n",
    "        value = self.value(value) # (batch_size, max_words, d_model->512)\n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) dot (batch_size, h, d_k, max_words)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(self.d_k)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # concat\n",
    "        interacted = self.concat(context)\n",
    "        return interacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_layers, word2idx):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab = len(word2idx)\n",
    "        self.embed = Embeddings(self.vocab, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab)\n",
    "    \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embedding = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embedding, src_mask)\n",
    "        return src_embeddings\n",
    "\n",
    "    def decode(self, target_words, target_mask, src_embedding, src_mask):\n",
    "        target_embedding = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            target_embedding = layer(target_embedding, src_embedding, src_mask, target_mask)\n",
    "        return target_embedding\n",
    "\n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded))\n",
    "        # F.log_softmax needed for KL Divergence Loss\n",
    "        # Not needed if using cross entropy loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.lr = lr\n",
    "        # Update Weights\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, smooth):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction: (batch_size, max_words, vocab_size)\n",
    "        target, mask: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "layers = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 1\n",
    "\n",
    "with open('word2idx.json', 'r') as j:\n",
    "    word2idx = json.load(j)\n",
    "\n",
    "transformer = Transformer(d_model=d_model, heads=heads, num_layers=layers, word2idx=word2idx)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size=d_model, warmup_steps=4000, optimizer=adam_optimizer)\n",
    "criterion = LossWithLabelSmoothing(size=len(word2idx), smooth = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        samples = question.shape[0]\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "        \n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "        \n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        \n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: [{epoch}][{i}/{len(train_loader)}]\\tLoss: {sum_loss / count:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word2idx):\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word2idx['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device) # (1, 1)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[0]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        # decoded is of shape (1, 1, word_size)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        # predictions is of shape (1, vocab_size)\n",
    "        _, next_word = torch.max(predictions, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1) # (1, step + 2)\n",
    "    \n",
    "    words = words.squeeze(0).tolist()\n",
    "    sen_idx = [w for w in words if w not in {word2idx['<start>']}]\n",
    "    sentence = ' '.join([idx2word[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    state = {\n",
    "        'epoch': epoch, \n",
    "        'transformer': transformer, \n",
    "        'transformer_optimizer': transformer_optimizer\n",
    "    }\n",
    "    torch.save(state, 'checkpoint_' + str(epoch) + '.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint_0.tar')\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \")\n",
    "    if not question:\n",
    "        break\n",
    "    max_len = input(\"Enter max words to be generated: \")\n",
    "    enc_qus = [word2idx.get(word, word2idx['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word2idx)\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079571a2d98e69c1029e062bab5439d652473f4c8d06888b961d2434b5eb26a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
